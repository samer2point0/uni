{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - Getting Started\n",
    "\n",
    "### Gabriele Pergola - gabriele.pergola@warwick.ac.uk\n",
    "\n",
    "NLTK or Natural Language Tool Kit is a Python module that contains a number of resources for building natural language processing applications.\n",
    "\n",
    "The toolkit comes with a number of existing text corpora that you can use for building and training models out of the box. It also comes with useful \"lexical resource\" datasets that can be used to augment and supplement your application.\n",
    "\n",
    "More information about the following exercises are available in the [Chapter 1](http://www.nltk.org/book/ch01.html#sec-computing-with-language-texts-and-words) of the NLTK book.\n",
    "\n",
    "## NLTK text resources\n",
    "\n",
    "NLTK comes with a number of resoures. It is very easy to import them and use them to build NLP tools. <br>\n",
    "Let's start by listing NLTK resources available to us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, let's download NLTK corpora\n",
    "import nltk\n",
    "\n",
    "nltk.download('book')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.book import *\n",
    "# Print the list of the available books\n",
    "texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK Text object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Text object is a wrapper for a list of tokens representing the documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text1.tokens[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK Text objects are stored in a way such that it is very easy to do some common NLP tasks on them.\n",
    "\n",
    "## Concordance and similarity\n",
    "\n",
    "The NLTK `concordance()` function generates a list of all of the instances of a particular word in context, this allows you to see how the word is being used. Let's try this on the Moby Dick text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that \"monstrous\" is often used in the context of size and whales. I guess this is no surprise given the book we're reading.\n",
    "\n",
    "Another function we can use here is the `similar()` function. This uses the context of the word to find words used in similar context. I.e. it looks for the words surrounding \"monstrous\" such as <i> \"most _ size\" </i> or <i>\"the _ pictures\"</i> and tries to find other words that match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.similar(\"monstrous\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although perhaps a little tenuously related, these are all adjectives that do roughly fit the contexts described above.\n",
    "\n",
    "We can also look at the other words used in the book and how frequently they are used.\n",
    "\n",
    "## Frequency Distributions and NLTK\n",
    "\n",
    "NLTK provides a special `dictionary` that counts occurrences of items in a list. It is called `FreqDist` and allows you to plot graphs.\n",
    "\n",
    "Let's examine the words in Moby dick with a frequency dist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist(text1)\n",
    "\n",
    "print(\"--- Sample of word frequencies ---\")\n",
    "print(\"'the': \", f[\"the\"])\n",
    "print(\"'whale': \", f[\"whale\"])\n",
    "print(\"'monstrous': \", f[\"monstrous\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting but unfortunately a lot of the words that are being flagged up as the most frequent are common words like 'the', 'of', 'and', 'to' and more. These are what we call <b>stopwords</b> - words common to almost all documents and as such, that provide no value to an analyst. We want to filter these out if we can. \n",
    "\n",
    "Thankfully NLTK comes with a stopwords list too. All we need to do is filter moby dick using this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as StopwordsLoader\n",
    "\n",
    "stopwords = StopwordsLoader.words() + [':','?','!','\"','--','-', \"'\", '.\"', ';','.',',']\n",
    "\n",
    "f = FreqDist([ x for x in text1 if x not in stopwords ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and plot the most frequent words except stopwords\n",
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more interesting and informative. This plot really helps paint a picture of the plots and themes of the book. We are still seeing a number of words that are not descriptive. Let's introduce a rule that filters out words shorter than 5 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist([ x for x in text1 if (x not in stopwords and len(x) > 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print and plot the most frequent words longer than 4 characters except stopwords\n",
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "Collocations are sequences of words that occur together more frequently than normal in a passage of text. For example \"red wine\" or \"single mum\" or \"slim build\". Often collocations describe well known phrases or idioms or are compound nouns. The NLTK book describes collocations as being resistant to substitution with words that have similar senses - e.g. maroon wine just doesn't seem the same as red wine.\n",
    "\n",
    "We find collocations by identifying the most frequent bigrams in the text. Bigrams are just pairs of words that occur next to each other. Like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "print (list(bigrams(\"Moby Dick is about whales and human beings!\".split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has a built in collocations function that can be run on Moby Dick like so. It calculates the most common bigrams in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The collocations here are very specific to the book - Moby Dick. This gives us a great idea of the sorts of concepts and ideas that are important in Moby Dick.\n",
    "\n",
    "## Using your own text with NLTK\n",
    "\n",
    "It's great that NLTK comes with so many resources, but how do you go about using your own corpus? If you have a series of plain text files, like a movie review dataset, this is very simple. We use a `PlainTextCorpusReader` to enable NLTK to ingest and preprocess the corpus and allow us to do exercises like the ones above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possibile to create a Text object from a text file on your filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "# Reading from disk and creating the Text object\n",
    "my_local_corpus = PlaintextCorpusReader(\"Datasets/movie_reviews\", \"\\w+\\.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We have now loaded the movie review corpus into NLTK. It can be split into words and sentences automatically for us. Let's examine the overall word frequency across the movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist([ x for x in my_local_corpus.words() if (x not in stopwords and len(x) > 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f.most_common(20))\n",
    "\n",
    "f.plot(20, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Not really any surprises here. Lots of words that make sense in a movie review context. Let's try doing collocations again.\n",
    "\n",
    "As this is a custom corpus we will have to do a bit of set up this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(my_local_corpus.words())\n",
    "\n",
    "# Filter collocations appearing less than 3 times\n",
    "finder.apply_freq_filter(3)\n",
    "\n",
    "print (finder.nbest(bigram_measures.pmi, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the already implemented function for extracting collocation using an NLTK Text object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the implementation of \"collocations()\" in a Text object, compare the differences:\n",
    "# http://www.nltk.org/_modules/nltk/text.html\n",
    "\n",
    "# Convert to NLTK text object\n",
    "my_local_corpus_asText  = nltk.Text(my_local_corpus.words())\n",
    "\n",
    "print(my_local_corpus_asText.collocations())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much more interesting. What we start to see are names of actors and other crew members from movies under review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reading and more activities\n",
    "\n",
    "NLTK provides a huge amount of scope for NLP experiments and text mining. For more ideas and guidance it is worth reading the [NLTK book](http://www.nltk.org/book/) online."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
