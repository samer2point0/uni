{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a program that analyses a news corpus and prints off some interesting stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First importing the neccassary modules for the program adn downloading nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import jsonlines\n",
    "import nltk\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving lists of negative and positive words in global variables nWords and pWords respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./signal-news1/opinion-lexicon-English/positive-words.txt', 'r') as pWF:\n",
    "    pWords=pWF.read().split()\n",
    "with open('./signal-news1/opinion-lexicon-English/negative-words.txt', 'r') as nWF:\n",
    "    nWords=nWF.read().split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function prep takes a piece of text and removes urls, nonalphanumeric characters, numbers and all one lettter words and then returns the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep(text):\n",
    "    urls=re.compile(r'http[\\w\\W]*\\b')\n",
    "    text=urls.sub('',text)\n",
    "    nonalpha=re.compile(r'[^a-z0-9\\s]+')\n",
    "    text=nonalpha.sub('',text)\n",
    "    num=re.compile(r'\\b[0-9]+\\b')\n",
    "    text=num.sub('',text)\n",
    "    short=re.compile(r'\\b[a-z]{1}\\b')\n",
    "    text=short.sub('',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatize takes a list of words and returns lemmatized version of text as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    wnl=nltk.WordNetLemmatizer()\n",
    "    return [wnl.lemmatize(word) for word in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function pVn takes a peice of text and counts the number of positive and negative words in it and stores the value in the pnCount dictionary, it also adds one to the number of positive articles if the total number of positive words was larger than negative words and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pVn(pnCount,artText):\n",
    "    PWs=0\n",
    "    NWs=0\n",
    "    #add number of positive words in article text to the PWs count\n",
    "    for word in pWords:\n",
    "        if word in artText:\n",
    "            PWs=PWs+artText[word]\n",
    "            del artText[word]\n",
    "    #add number of negattive words in article text to the PWs count\n",
    "    for word in nWords:\n",
    "        if word in artText:\n",
    "            NWs=NWs+artText[word]\n",
    "    #edit the global variable pnCount by adding number of new positive and negative words seen\n",
    "    pnCount['pCount']=pnCount['pCount']+PWs\n",
    "    pnCount['nCount']=pnCount['nCount']+NWs\n",
    "\n",
    "    #classify article into positive or negative and edit pnCount['pArticles'/'nArticles'] accordingly\n",
    "    if(PWs>NWs):\n",
    "        pnCount['pArticles']=pnCount['pArticles']+1\n",
    "    elif(NWs>PWs):\n",
    "        pnCount['nArticles']=pnCount['nArticles']+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function topTrigram takes a sorted list of trigrams and a bigram and returns the most likely trigram given the bigram based only on seen trigrams (no smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topTrigram(trigrams, bigram):\n",
    "    triList=[]\n",
    "    tempTri=trigrams\n",
    "    #Binary search for bigram in sorted trigrams list\n",
    "    while True:\n",
    "        #if more than 1 element left split list according to bigram possition\n",
    "        if len(tempTri)>1 and tempTri[int(len(tempTri)/2)][0][:2] < bigram:\n",
    "            tempTri=tempTri[int(len(tempTri)/2):]\n",
    "        elif len(tempTri)>1 and tempTri[int(len(tempTri)/2)][0][:2] > bigram:\n",
    "            tempTri=tempTri[:int(len(tempTri)/2)]\n",
    "        #runs if bigram found or search exhausted\n",
    "        else:\n",
    "            #if bigram not found return None\n",
    "            if len(tempTri)<1 or tempTri[int(len(tempTri)/2)][0][:2] != bigram:\n",
    "                return None\n",
    "\n",
    "            i,j=0,1\n",
    "            #add all trigrams starting with bigram to the triList\n",
    "            while int(len(tempTri)/2)-i>0 and tempTri[int(len(tempTri)/2)-i][0][:2]==bigram:\n",
    "                triList.insert(0, tempTri[int(len(tempTri)/2)-i])\n",
    "                i=i+1\n",
    "            while int(len(tempTri)/2)+j<len(tempTri) and tempTri[int(len(tempTri)/2)+j][0][:2]==bigram:\n",
    "                triList.append(tempTri[int(len(tempTri)/2)+j])\n",
    "                j=j+1\n",
    "            break\n",
    "\n",
    "    #find the maximumm occuring trigram and return it\n",
    "    topTri=max(triList, key=lambda x: x[1])[0]\n",
    "    return topTri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finish sentence takes a sorted list of trigrams, an ngram and a sentence length(n) it uses a trigram model to recursively find out the next most likely n words based on the trigrams seens so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finishSentence(trigrams, ngram, n):\n",
    "    #find bigram out of sentence passed\n",
    "    bigram=tuple(ngram.split()[-2:])\n",
    "    topTri=topTrigram(trigrams,bigram)\n",
    "    #if no trigram starting with bigram found return the sentence constructed so far\n",
    "    if not topTri:\n",
    "        return ngram\n",
    "    #find most liekely next word and add it to sentence so far\n",
    "    topWord=topTri[-1]\n",
    "    ngram=ngram+' '+topWord\n",
    "\n",
    "    #if the length of the sentence so far is less than n call finishSentence witht the new ngram\n",
    "    if len(ngram.split())<n:\n",
    "        sentence=finishSentence(trigrams, ngram, n)\n",
    "    else:#if length of ngram is n return ngram\n",
    "        sentence=ngram\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function preplexity takes list of trigrams and bigrams from training corpus and a list of trigrams from the test corpus and evaluates the preplexity using laplace smoothing and a second order markov assumption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preplexity(trainTrigrams, trainBigrams, testTrigrams):\n",
    "    ppxsum=0\n",
    "    N=0\n",
    "    V=len(trainBigrams)\n",
    "    #trigramCount, bigramCount=0,0\n",
    "    for trigram in testTrigrams:\n",
    "        #find occurances of bigram in training data and adding V for laplace smoothing\n",
    "        bigram=trigram[:2]\n",
    "        try:\n",
    "            bigramCount=trainBigrams[bigram]+V\n",
    "        except (KeyError):\n",
    "            bigramCount=V\n",
    "\n",
    "        #find occurances of trigram in training data and adding 1 for laplace smoothing\n",
    "        try:\n",
    "            trigramCount=trainTrigrams[trigram]+1\n",
    "        except (KeyError):\n",
    "            trigramCount=1\n",
    "\n",
    "        #calculate the probablity of each trigram and add it to sum\n",
    "        prTriGivenBi=trigramCount/bigramCount\n",
    "        #add no of time trigram occured in test data to total no of words\n",
    "        testTriCount=testTrigrams[trigram]\n",
    "        N=N+testTriCount\n",
    "        #using log and addition to cope with very large numbers\n",
    "        ppxsum=ppxsum+math.log(prTriGivenBi,2)*testTriCount\n",
    "\n",
    "    #return the preplexity\n",
    "    return pow(2,-ppxsum/N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main function of program \n",
    "\n",
    "starts by initialising variables including the pnCount mentioned earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    #initialising variables\n",
    "    #pnCount dictionary saves the number of negative/positive words and articles\n",
    "    pnCount={'pArticles':0,'nArticles':0, 'nCount':0, 'pCount':0}\n",
    "    T,V=[],[]\n",
    "    breakFlag=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next code block reads from file and preprocesses it before building language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    " #reading from file formatting it and storing it in T\n",
    "    with jsonlines.open('./signal-news1/signal-news1.jsonl', 'r') as reader:\n",
    "        i=0\n",
    "        for line in reader:\n",
    "            i=i+1\n",
    "            text=line['content'].lower()\n",
    "\n",
    "            #preocess text, lemmatize it and add it ot list\n",
    "            proctext=prep(text).split()\n",
    "            del text\n",
    "            lemT=lemmatize(proctext)\n",
    "            del proctext\n",
    "            T.extend(lemT)\n",
    "\n",
    "            #flag to deperate first 16000 line from test corpus\n",
    "            if(i==16000):\n",
    "                breakFlag=len(T)\n",
    "\n",
    "            #create temporary article vocabulary and pass it to pVn that counts positive vs negative words and artcles\n",
    "            tempV=collections.Counter(lemT)\n",
    "            pVn(pnCount,tempV)\n",
    "            del lemT,tempV\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vocabulary size and token size and print them, then print total number of positive/negative words and positive negative articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size is:  123423 \n",
      "token size is:  5653079\n",
      "there are  6377  negative articles \n",
      "there are  10810 positive articles\n",
      "there are  128773 negative words \n",
      "there are  169538 positive words\n"
     ]
    }
   ],
   "source": [
    "    vCount=len(set(T))\n",
    "    tCount=len(T)\n",
    "    print('vocabulary size is: ',vCount,'\\ntoken size is: ', tCount)\n",
    "    print('there are ',pnCount['nArticles'], ' negative articles', '\\nthere are ',pnCount['pArticles'], 'positive articles')\n",
    "    print('there are ',pnCount['nCount'], 'negative words', '\\nthere are ',pnCount['pCount'], 'positive words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section sorts the trigrams by occurances and lists the top 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the top 25 trigrams are: \n",
      " [(('one', 'of', 'the'), 2422), (('on', 'share', 'of'), 2095), (('on', 'the', 'stock'), 1562), (('in', 'research', 'report'), 1415), (('a', 'well', 'a'), 1399), (('in', 'research', 'note'), 1373), (('for', 'the', 'quarter'), 1221), (('the', 'united', 'state'), 1196), (('average', 'price', 'of'), 1193), (('research', 'report', 'on'), 1177), (('research', 'note', 'on'), 1138), (('share', 'of', 'the'), 1131), (('the', 'end', 'of'), 1125), (('in', 'report', 'on'), 1124), (('earnings', 'per', 'share'), 1121), (('cell', 'phone', 'plan'), 1073), (('phone', 'plan', 'detail'), 1070), (('according', 'to', 'the'), 1056), (('of', 'the', 'company'), 1056), (('buy', 'rating', 'to'), 1015), (('moving', 'average', 'price'), 995), (('appeared', 'first', 'on'), 994), (('day', 'moving', 'average'), 991), (('price', 'target', 'on'), 977), (('part', 'of', 'the'), 930)]\n"
     ]
    }
   ],
   "source": [
    "    tri= collections.Counter(nltk.trigrams(T))\n",
    "    topTri=sorted(tri.items(), key=lambda x: x[1], reverse=True)\n",
    "    del tri\n",
    "    print('the top 25 trigrams are: \\n',topTri[:25])\n",
    "    del topTri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section lists the trigrams and bigrams of the training dataset and also the trigrams of the test corpus.\n",
    "a sorted version of the training set trigrams(Sortedtri1) is then passed to the finishsentence function which returns the n length sentence finished based on the trigrams langugae model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the generated sentence starting with (is this) is:   is this the company ha market capitalization of billion and\n"
     ]
    }
   ],
   "source": [
    "    tri1=collections.Counter(nltk.trigrams(T[:breakFlag]))\n",
    "    bi1=collections.Counter(nltk.bigrams(T[:breakFlag]))\n",
    "    tri2=collections.Counter(nltk.trigrams(T[breakFlag:]))\n",
    "    del T\n",
    "    sortedTri1=sorted(tri1.items(), key=lambda x: x[0])\n",
    "    print('the generated sentence starting with (is this) is:  ', finishSentence(sortedTri1, 'is this', 10))\n",
    "    del sortedTri1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally this section calculates the preplexity of the language model on the rest of the jsonlines rows, given trigrams and bigrams on test and training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the preplexity of the language model is:  521201.3724973951\n"
     ]
    }
   ],
   "source": [
    "    print('the preplexity of the language model is: ', preplexity(tri1, bi1, tri2))\n",
    "    del tri1, bi1, tri2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
