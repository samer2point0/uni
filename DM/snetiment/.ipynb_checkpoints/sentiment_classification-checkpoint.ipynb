{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Classification: classifying IMDB reviews\n",
    "\n",
    "In this task, you will learn how to process text data and how to train neural networks with limited input text data using pre-trained embeddings for sentiment classification (classifying a review document as \"positive\" or \"negative\" based solely on the text content of the review)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `Embedding` layer in Keras to represent text input. The `Embedding` layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes as input integers, then looks up these integers into an internal dictionary, and finally returns the associated vectors. It's effectively a dictionary lookup.\n",
    "\n",
    "The `Embedding` layer takes as input a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have  shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
    "\n",
    "This layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
    "\n",
    "You can instantiate the `Embedding` layer by randomly initialising its weights (its internal dictionary of token vectors). During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for. You can also instantiate the `Embedding` layer by intialising its weights using the pre-trained word embeddings, such as GloVe word embeddings pretrained from Wikipedia articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Download the IMDB data as raw text\n",
    "\n",
    "First, create a \"data\" directory, then head to `http://ai.stanford.edu/~amaas/data/sentiment/` and download the raw IMDB dataset (if the URL isn't working anymore, just Google \"IMDB dataset\"). Save it into the \"data\" directory. Uncompress it. Store the individual reviews into a list of strings, one string per review, and also collect the review labels (positive / negative) into a separate `labels` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# write your code here\n",
    "\n",
    "train_l=[]\n",
    "test_l=[]\n",
    "tk=[] #contains both trainging and testing datasets\n",
    "\n",
    "#read the files in the train folder and label them according to subfolder\n",
    "train_path='data/aclImdb/train/'\n",
    "#2000 reviews half positive and half negative\n",
    "train_pos_filenames = [train_path+'pos/'+i for i in os.listdir(train_path+'pos')][:1000]\n",
    "train_neg_filenames= [train_path+'neg/'+i for i in os.listdir(train_path+'neg')][:1000]\n",
    "for i in range(2*len(train_pos_filenames)):\n",
    "    if i%2==0:\n",
    "        t=train_pos_filenames\n",
    "        label=0\n",
    "    else:\n",
    "        t=train_neg_filenames\n",
    "        label=1\n",
    "    with open(t[i//2], 'r') as f:\n",
    "        text=f.read()\n",
    "        train_l.append((text,label))\n",
    "        tk.append(text)\n",
    "\n",
    "        \n",
    "#read the files in the test folder and label them according to subfolder\n",
    "test_path='data/aclImdb/test/'\n",
    "#2000 reviews half positive and half negative\n",
    "test_pos_filenames = [test_path+'pos/'+i for i in os.listdir(test_path+'pos')][:1000]\n",
    "test_neg_filenames= [test_path+'neg/'+i for i in os.listdir(test_path+'neg')][:1000]\n",
    "for i in range(2*len(test_pos_filenames)):\n",
    "    if i%2==0:\n",
    "        t=test_pos_filenames\n",
    "        label=0\n",
    "    else:\n",
    "        t=test_neg_filenames\n",
    "        label=1\n",
    "    with open(t[i//2], 'r') as f:\n",
    "        text=f.read()\n",
    "        test_l.append((text,label))\n",
    "        tk.append(text)\n",
    "\n",
    "#shuffle samples\n",
    "random.shuffle(train_l)\n",
    "random.shuffle(test_l)\n",
    "\n",
    "#split training into training and validation\n",
    "train_x, train_y = zip(*train_l)\n",
    "test_x, test_y = zip(*test_l)\n",
    "valid_x,valid_y=train_x[:len(train_x)*1//10],train_y[:len(train_y)*1//10]\n",
    "train_x,train_y=train_x[len(train_x)*1//10:],train_y[len(train_y)*1//10:]\n",
    "\n",
    "#transform outputs to numpy arrays \n",
    "train_y=np.array(train_y)\n",
    "valid_y=np.array(valid_y)\n",
    "test_y=np.array(test_y)\n",
    "#inputs will undergo more processing first\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-process the review documents \n",
    "\n",
    "Pre-process review documents by tokenisation and split the data into the training and testing sets. You can restrict the training data to the first 1000 reviews and only consider the top 5,000 words in the dataset. You can also cut reviews after 100 words (that is, each review contains a maximum of 100 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# write your code here\n",
    "\n",
    "#the list was compiled by first looking into the words with the top tfidf rates\n",
    "#removed words indicating negation(e.g. not) and exagration(e.g. very) from list \n",
    "stopwords=['the', 'and', 'of', 'to', 'is', 'br', 'it', 'in', 'this', 'that', 'was','with', 'as', 'for', 'you', \n",
    "'on', 'are', 'he', 'have', 'his', 'be', 'one', 'at', 'they', 'all', 'who', 'by', 'from', 'so', 'an', 'off',\n",
    "'there', 'her', 'if','out', 'or', 'about', 'just', 'has', 'what', 'can', 'some', 'when', 'she', 'up', 'my',\n",
    "'their', 'which', 'me', 'were', 'had', 'we', 'well', 'get', 'than', 'because', 'will', 'did', 'your','over'\n",
    "'been', 'its', 'other', 'do', 'also', 'into''him', 'how', 'too', 'them', 'after', 'any', 'then', 'before','those']\n",
    "\n",
    "#find the top 5000 words with the highest avg tdidf scores across all reviews\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words=stopwords)\n",
    "idf=tfidf.fit_transform(tk)\n",
    "#transform idf matrix into a 2d matrix and list the mean tdidf for each word\n",
    "words_tdidf=idf.todense().mean(axis=0).tolist()[0]\n",
    "#match each tdidf score with the word and put in a list of tuples\n",
    "top_tokens=sorted(tfidf.vocabulary_)\n",
    "d=list(zip(top_tokens,words_tdidf))\n",
    "\n",
    "#initilise tokenizer and set word_counts attributes and word_index\n",
    "token=Tokenizer()\n",
    "\n",
    "token.word_counts=d #replace word counts with word tdidf score \n",
    "token.word_index=tfidf.vocabulary_ #set tokeniser vocubulary to idf vocabulary\n",
    "\n",
    "#transform text to sequence of strings\n",
    "tk_train_x=token.texts_to_sequences(train_x)\n",
    "tk_valid_x=token.texts_to_sequences(valid_x)\n",
    "tk_test_x=token.texts_to_sequences(test_x)\n",
    "\n",
    "#pad the input to have 100 items \n",
    "tk_train_x=pad_sequences(tk_train_x, maxlen = 100, padding = \"post\", truncating = \"post\", value = 0)\n",
    "tk_valid_x=pad_sequences(tk_valid_x, maxlen = 100, padding = \"post\", truncating = \"post\", value = 0)\n",
    "tk_test_x=pad_sequences(tk_test_x, maxlen = 100, padding = \"post\", truncating = \"post\", value = 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) Download the GloVe word embeddings and map each word in the dataset into its pre-trained GloVe word embedding.\n",
    "\n",
    "\n",
    "First go to `https://nlp.stanford.edu/projects/glove/` and download the pre-trained \n",
    "embeddings from 2014 English Wikipedia into the \"data\" directory. It's a 822MB zip file named `glove.6B.zip`, containing 100-dimensional embedding vectors for \n",
    "400,000 words (or non-word tokens). Un-zip it.\n",
    "\n",
    "Parse the un-zipped file (it's a `txt` file) to build an index mapping words (as strings) to their vector representation (as number vectors).\n",
    "\n",
    "Build an embedding matrix that will be loaded into an `Embedding` layer later. It must be a matrix of shape `(max_words, embedding_dim)`, where each entry `i` contains the `embedding_dim`-dimensional vector for the word of index `i` in our reference word index \n",
    "(built during tokenization). Note that the index `0` is not supposed to stand for any word or token -- it's a placeholder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "\n",
    "#initialise glove file path and embedding matrix \n",
    "glove_path='./glove.6B.100d.txt'\n",
    "wn=len(top_tokens)+1\n",
    "embed_matrix=np.zeros((wn,100))\n",
    "\n",
    "#open glove text file and save it to content\n",
    "with open(glove_path, encoding=\"utf8\" ) as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "#for each line check if word in list of top words \n",
    "#if it is then add the word embedding 100d vector to the embed_matrix\n",
    "for line in content:\n",
    "    splitLine = line.split()\n",
    "    word = splitLine[0]\n",
    "    if word in top_tokens:\n",
    "        index=top_tokens.index(word)\n",
    "        embedding = np.asarray(splitLine[1:])\n",
    "        embed_matrix[index+1] = embedding\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Build and train a simple Sequential model\n",
    "\n",
    "The model contains an Embedding Layer with maximum number of tokens to be 10,000 and embedding dimensionality as 100. Initialise the Embedding Layer with the pre-trained GloVe word vectors. Set the maximum length of each review to 100. Flatten the 3D embedding output to 2D and add a Dense Layer which is the classifier. Train the model with a 'rmsprop' optimiser. You need to freeze the embedding layer by setting its `trainable` attribute to `False` so that its weights will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_13 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_13 (Embedding)     (None, 100, 100)          500100    \n",
      "_________________________________________________________________\n",
      "flatten_13 (Flatten)         (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 820,165\n",
      "Trainable params: 320,065\n",
      "Non-trainable params: 500,100\n",
      "_________________________________________________________________\n",
      "Train on 1800 samples, validate on 200 samples\n",
      "Epoch 1/20\n",
      "1800/1800 [==============================] - 3s 2ms/step - loss: 0.7240 - val_loss: 0.6966\n",
      "Epoch 2/20\n",
      "1800/1800 [==============================] - 2s 911us/step - loss: 0.5844 - val_loss: 0.8027\n",
      "Epoch 3/20\n",
      "1800/1800 [==============================] - 1s 750us/step - loss: 0.3095 - val_loss: 0.9353\n",
      "Epoch 4/20\n",
      "1800/1800 [==============================] - 2s 852us/step - loss: 0.1425 - val_loss: 1.1117\n",
      "Epoch 5/20\n",
      "1800/1800 [==============================] - 1s 744us/step - loss: 0.0533 - val_loss: 1.2846\n",
      "Epoch 6/20\n",
      "1800/1800 [==============================] - 2s 898us/step - loss: 0.0262 - val_loss: 1.4091\n",
      "Epoch 7/20\n",
      "1800/1800 [==============================] - 2s 964us/step - loss: 0.0124 - val_loss: 1.4069\n",
      "Epoch 8/20\n",
      "1800/1800 [==============================] - 1s 784us/step - loss: 0.0073 - val_loss: 1.7012\n",
      "Epoch 9/20\n",
      "1800/1800 [==============================] - 2s 911us/step - loss: 0.0082 - val_loss: 1.7256\n",
      "Epoch 10/20\n",
      "1800/1800 [==============================] - 1s 823us/step - loss: 0.0046 - val_loss: 1.9993\n",
      "Epoch 11/20\n",
      "1800/1800 [==============================] - 2s 882us/step - loss: 0.0073 - val_loss: 2.0787\n",
      "Epoch 12/20\n",
      "1800/1800 [==============================] - 2s 922us/step - loss: 0.0050 - val_loss: 2.2286\n",
      "Epoch 13/20\n",
      "1800/1800 [==============================] - 1s 668us/step - loss: 0.0032 - val_loss: 2.3028\n",
      "Epoch 14/20\n",
      "1800/1800 [==============================] - 1s 754us/step - loss: 0.0013 - val_loss: 2.7670\n",
      "Epoch 15/20\n",
      "1800/1800 [==============================] - 2s 853us/step - loss: 0.0032 - val_loss: 2.6528\n",
      "Epoch 16/20\n",
      "1800/1800 [==============================] - 2s 911us/step - loss: 0.0027 - val_loss: 2.7356\n",
      "Epoch 17/20\n",
      "1800/1800 [==============================] - 1s 664us/step - loss: 0.0046 - val_loss: 2.8281\n",
      "Epoch 18/20\n",
      "1800/1800 [==============================] - 2s 889us/step - loss: 0.0028 - val_loss: 2.9292\n",
      "Epoch 19/20\n",
      "1800/1800 [==============================] - 2s 854us/step - loss: 0.0028 - val_loss: 3.1703\n",
      "Epoch 20/20\n",
      "1800/1800 [==============================] - 1s 700us/step - loss: 0.0048 - val_loss: 2.9740\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Input, \n",
    "\n",
    "# write your code here\n",
    "\n",
    "#create the imbed layer with the weights defined by the embedidng matrix and trainable attribute to false\n",
    "embed_layer = Embedding(input_dim=embed_matrix.shape[0],output_dim=embed_matrix.shape[1], \n",
    "                        input_length=100, weights=[embed_matrix],trainable=False, mask_zero=False)\n",
    "\n",
    "#create a sequential network model with embedding layer and a feedforeward layer with 32 neurons\n",
    "i = Input(shape=(100,), dtype='int32')\n",
    "x = embed_layer(i)\n",
    "x = Flatten()(x)\n",
    "o = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=i, outputs=o)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "# fit network\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(tk_train_x, train_y, verbose=1, batch_size=32, epochs=20,\n",
    "                    validation_data=(tk_valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Plot the training and validation loss and accuracies and evaluate the trained model on the test set.\n",
    "\n",
    "What do you observe from the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.48      0.49      1000\n",
      "           1       0.50      0.51      0.50      1000\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      2000\n",
      "   macro avg       0.49      0.49      0.49      2000\n",
      "weighted avg       0.49      0.49      0.49      2000\n",
      "\n",
      "0.495\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# write your code here\n",
    "#plot training and validation loss and accuracy histoties\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#predict the sentiment for test dataset and print the classification report \n",
    "pred_test_y=model.predict(np.array(tk_test_x), batch_size=32)\n",
    "print(classification_report(test_y,pred_test_y.round()))\n",
    "print(accuracy_score(test_y,pred_test_y.round()))\n",
    "\n",
    "#from the results we see that the network overfits grossly on the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Add an LSTM layer into the simple neural network architecture and re-train the model on the training set, plot the training and validation loss/accuracies, also evaluate the trained model on the test set and report the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, regularizers, Dropout\n",
    "\n",
    "# write your code here\n",
    "embed_layer = Embedding(input_dim=embed_matrix.shape[0],output_dim=embed_matrix.shape[1], \n",
    "                        input_length=100, weights=[embed_matrix],trainable=False, mask_zero=True)\n",
    "\n",
    "\n",
    "#create a sequential network model with embedding layer and a LSTM layer with 32 neurons\n",
    "#adding weights regulizer and dropout to limit overfitting\n",
    "i = Input(shape=(100,), dtype='int32')\n",
    "x = embed_layer(i)\n",
    "x = LSTM(24, kernel_regularizer = regularizers.l2(l = 0.05))(x)\n",
    "x =Dropout(0.4)(x)\n",
    "#x = Flatten()(x)\n",
    "o = Dense(1,activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=i, outputs=o)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "model.summary()\n",
    "\n",
    "# fit network\n",
    "history = model.fit(tk_train_x, train_y, verbose=1, batch_size=64, epochs=20,\n",
    "                    validation_data=(tk_valid_x, valid_y))\n",
    "\n",
    "#plot training and validation loss and accuracy histoties\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#predict the sentiment for test dataset and print the classification report \n",
    "pred_test_y=model.predict(np.array(tk_test_x), batch_size=32)\n",
    "print(classification_report(test_y,pred_test_y.round()))\n",
    "print(accuracy_score(test_y,pred_test_y.round()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
